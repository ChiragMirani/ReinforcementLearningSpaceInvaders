{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d068efa",
   "metadata": {},
   "source": [
    "* Written and coded by: Chirag Mirani\n",
    "* On Tuesday, January 25, 2022\n",
    "\n",
    "# 1. Importing Required Libraries\n",
    "* Key library here is Openai Gym, which allows us to import OpenAI environments\n",
    "* This \"from IPython.display import clear_output\" is used to clear output from Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42fbd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541a9ac",
   "metadata": {},
   "source": [
    "# 2. Importing Taxi-v3 environment\n",
    "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1d997d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad517d8",
   "metadata": {},
   "source": [
    "# 3. Go through ten random action episode in the Taxi-v3 enviroment\n",
    "* 1. 10 episodes are iterated through via for loop\n",
    "* 2. We initialize each episode\n",
    "* 3. We perform action until we are done for each game episode\n",
    "    * 1. We render the environment\n",
    "    * 2. We take a random action and store the next state, reward, done (True or false) and info for each action taken\n",
    "    * 3. We accumulate our total score, which is a running sum of reward received for each action (through one complete episode)\n",
    "    * 4. We clear the output after each action to display the next screen. \n",
    "    * 5. After each epiosde, we display the total score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cfe6b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9\n",
      "Score: -713\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "\n",
    "for episode in range (1, episodes):\n",
    "    state = env.reset()\n",
    "    done= False\n",
    "    score =0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        state, reward, done, info = env.step(env.action_space.sample())\n",
    "        score+=reward\n",
    "        clear_output(wait=True)\n",
    "    print ('Episode: {}\\nScore: {}'.format (episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb54679",
   "metadata": {},
   "source": [
    "# Here we build our Q-table\n",
    "# What?? What the heck is a Q-table???\n",
    "* Q-table is a fancy name for a look up table for the environment that the agent can reference and update.  Specifically, along the rows you have different states encountered and along the columns is an action the agent can take.  The cells will hold the value received for state,action pair.  Interesting thought here is that how would the agent know the size of this q-table at inception because he hasn't explored the environment. \n",
    "* Hence Q-table is initialized to state x actions matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9a30a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = env.action_space.n\n",
    "state = env.observation_space.n\n",
    "\n",
    "# initialized our Q-table\n",
    "q_table = np.zeros((state, actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "884a06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for Q-learning\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate =0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate=1\n",
    "max_exploration_rate =1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate=0.001\n",
    "rewards_all_episodes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45ff50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Training Finished******\n"
     ]
    }
   ],
   "source": [
    "#Q-learning Algorithm\n",
    "\n",
    "for episodes in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    rewards_current_episode =0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        #Exploration vs Exploitation trade-off\n",
    "        exploration_treshold=np.random.uniform(0,1)\n",
    "        if exploration_treshold>exploration_rate:\n",
    "            action=np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #Update Q-Table\n",
    "        q_table[state, action] = learning_rate*(reward + discount_rate*np.max(q_table[new_state,:]))+q_table[state,action] \\\n",
    "                                -learning_rate*(q_table[state,action])\n",
    "       \n",
    "        state=new_state\n",
    "        rewards_current_episode +=reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "                \n",
    "    exploration_rate =min_exploration_rate+ \\\n",
    "                    (max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "print(\"***** Training Finished******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "743c7699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 4.72961412,  5.865136  ,  4.86149983,  6.15531319,  7.44936047,\n",
       "        -2.80174748],\n",
       "       [10.38150481, 11.65009807, 10.41929749, 11.6711564 , 12.88259705,\n",
       "         2.64633382],\n",
       "       ...,\n",
       "       [13.42805882, 14.8796532 , 13.49330736, 11.96069377,  4.41648867,\n",
       "         4.4819251 ],\n",
       "       [ 6.05846999,  7.95668135,  6.12379843,  7.73069972, -2.99643012,\n",
       "        -2.7902915 ],\n",
       "       [16.78791948, 15.69059773, 17.06967515, 18.56699302,  8.15224757,\n",
       "         8.11085919]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and print average reward per thousand episodes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
