{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f39ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Author: Chirag Mirani\n",
    "# Date: January 24, 2022\n",
    "\n",
    "# In this project, we are demonstrating how to load the OpenAI gym environment and take random actions.\n",
    "\n",
    "# we are importing gym library to render space invadors\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdd4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading space invaders pixel based environment\n",
    "# For pixel based environment one should use neural network\n",
    "env = gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d86041",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 2  # we are going to play 10 Space Invaders game episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2def79ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Score: 20.0\n"
     ]
    }
   ],
   "source": [
    "# start from Space Invaders game episode one and take random actions\n",
    "for episode in range(1, episodes):  \n",
    "    \n",
    "    # first reset the environment and game state\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #initialize score to 0..\n",
    "    score = 0\n",
    "    \n",
    "    #keep playing the game while the game is not done\n",
    "    while not done:\n",
    "        env.render()   #render the environment\n",
    "        # take an action and get the next state, reward, whether we are done and information about the environment\n",
    "        state, reward, done, info = env.step(env.action_space.sample())  # there are six actions we can take in space invaders. Start with random action\n",
    "        score +=reward  # store rewards\n",
    "    print ('Episode: {}\\nScore: {}'.format(episode, score))  # prints out score for each game episodes\n",
    "\n",
    "# don't forget to close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08001edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# for reference print out number of actions\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25de9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we are going to use a neural network to figure out optimal actions.\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# sequence of layers are needed\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# these are different types of layers you can have a neural network\n",
    "# Dense is a fully connected neural network\n",
    "# Flatten layer just flattens the previous neutral network into a one dimensional array\n",
    "# Convolution2d layer is needed to understand the images.  In this case, Space-Invador2.0\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "# Adam optimizer.  This optimizer will help us train the neural network such that it associates images with optimal actions\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de2b9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function, we are building our neural network. \n",
    "# these are pixels for our SpaceInvadersscreen (height, width, channels)\n",
    "# Colored images typically have three channels, for the pixel value at the (row, column) coordinate for the red, green, and blue components.\n",
    "\n",
    "# actions\n",
    "# Main input: input shape = our space invaders image\n",
    "# Main output: associate it with the six actions. \n",
    "def build_model(height, width, channels, actions):\n",
    "    # sequence of layers\n",
    "    model= Sequential()\n",
    "    # take in the image and condense the image..\n",
    "    # relu activation means the model is able to learn non-linear input/output relationships.  This allows for\n",
    "    # more complicated relationships. Training the neural network will work like this.  Take in an input and associate it \n",
    "    # with the best action.\n",
    "    model.add(Conv2D(32,(8,8), strides=(4,4), activation ='relu', input_shape=(3, height, width, channels)))\n",
    "    #output unit 64, (4,4) and strides = (2,2) are filters that will help the model understand the image better\n",
    "    model.add(Conv2D(64,(4,4), strides=(2,2),activation='relu'))\n",
    "    \n",
    "    #flatten image convolutions into one dimensional array\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # send the flattened image to 512 neurons\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    \n",
    "    # condense the associated image to 256 neurons\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    \n",
    "    # output an action..\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e6bc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73554519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are ready to learn from any screen we pass into the model. \n",
    "# above we have our neural network setup. \n",
    "# Take in the observation space\n",
    "height, width, channels = env.observation_space.shape\n",
    "#specify number of actions.  This will be the output of our neural network\n",
    "actions = env.action_space.n\n",
    "\n",
    "model = build_model(height, width, channels, actions)\n",
    "env.observation_space.shape\n",
    "\n",
    "#importing keras-rl2\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy=LinearAnnealedPolicy(EpsGreedyQPolicy(), attr=\"eps\", value_max=1, value_min=0.1,  value_test=0.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=2000, window_length =3)\n",
    "    dqn=DQNAgent(model=model, memory=memory, policy=policy, enable_dueling_network=True, dueling_type='avg', nb_actions=actions,\n",
    "                nb_steps_warmup=1000)\n",
    "    \n",
    "    return dqn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3921f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn =build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91baeb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb3a0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  999/10000 [=>............................] - ETA: 5:55 - reward: 0.2102done, took 39.825 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fedc2baca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env,nb_steps=1000,visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ac1a50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 3 episodes ...\n",
      "Episode 1: reward: 320.000, steps: 1115\n",
      "Episode 2: reward: 170.000, steps: 935\n",
      "Episode 3: reward: 170.000, steps: 801\n",
      "220.0\n"
     ]
    }
   ],
   "source": [
    "#env.close()\n",
    "env.reset()\n",
    "scores=dqn.test(env, nb_episodes=3, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01e57d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07e43140",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('models/ChiragsDQNSpaceInvadersRLModel.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e98a55c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'target_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_41048/976648254.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/ChiragsDQNSpaceInvadersRLModel.h5f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#env.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target_model_hard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chirag\\anaconda3\\envs\\my_rl_env\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mupdate_target_model_hard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_target_model_hard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'target_model'"
     ]
    }
   ],
   "source": [
    "dqn.load_weights('models/ChiragsDQNSpaceInvadersRLModel.h5f')\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfaf619c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        ...,\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22]],\n",
       "\n",
       "       [[80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        ...,\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22]],\n",
       "\n",
       "       [[80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        ...,\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22]]], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a455dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
